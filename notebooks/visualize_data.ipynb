{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the path with where data is stored\n",
    "# data_dir = Path(\"../../../ddemler/dima_stuff/wa_remake/May12_full_converted_dataset\")\n",
    "data_dir    = Path (\"../../../ddemler/dima_stuff/wa_remake/May_15_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_features.npy\n",
      "combined_labels.npy\n",
      "May_15zip_processed.zip\n"
     ]
    }
   ],
   "source": [
    "for f in data_dir.rglob(\"*\"):\n",
    "    print(f.relative_to(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The folder contains `combined_features.npy` and `combined_labels.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " feature sample (shape=(18, 16)):\n",
      "[[  128.    50.     0.   125.     8.     0.     8. 24574.     1.     2.\n",
      "      0.     8.     4.     1.     1.     0.]\n",
      " [  125.     8.     0.    63.    16.     0.     8. 24574.     1.     2.\n",
      "      0.    16.     5.     2.     2.     0.]\n",
      " [   63.    16.     0.    63.    16.     0.     8.     1.     1.     0.\n",
      "      2.     0.     0.     0.     0.     0.]\n",
      " [   63.    16.     0.    63.     8.     0.     8. 24574.     1.     2.\n",
      "      0.     8.     6.     1.     2.     0.]\n",
      " [   63.     8.     0.    63.     8.     0.     8.     1.     1.     0.\n",
      "      2.     0.     0.     0.     0.     0.]\n",
      " [   63.     8.     0.    63.     8.     0.     8. 24574.     1.     2.\n",
      "      0.     8.     2.     1.     2.     0.]\n",
      " [   63.     8.     0.    63.     8.     0.     8.     1.     1.     0.\n",
      "      4.     0.     0.     0.     0.     0.]\n",
      " [   63.     8.     0.    61.    16.     0.     8. 24574.     1.     2.\n",
      "      0.    16.     3.     1.     1.     0.]\n",
      " [   61.    16.     0.   976.     0.     0.     8.     1.     1.     8.\n",
      "      0.     0.     0.     0.     0.     0.]\n",
      " [  976.     0.     0.    16.     0.     0.     8. 24574.     1.     1.\n",
      "      0.     0.     0.     0.     0.     0.]\n",
      " [   16.     0.     0.    16.     0.     0.     8.     1.     1.     0.\n",
      "      3.     0.     0.     0.     0.     0.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]\n",
      " [   -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.    -1.\n",
      "     -1.    -1.    -1.    -1.    -1.    -1.]]\n",
      "\n",
      " label sample (shape=(6,)):\n",
      "[206465.  84615.  66731.    103.      6. 206466.]\n"
     ]
    }
   ],
   "source": [
    "feat = np.load(data_dir / \"combined_features.npy\")\n",
    "label  = np.load(data_dir / \"combined_labels.npy\")\n",
    "\n",
    "# no truncation\n",
    "np.set_printoptions(threshold=np.inf, suppress=True, precision=6)\n",
    "\n",
    "# print a sample of each\n",
    "print(\" feature sample (shape={}):\".format(feat[0].shape))\n",
    "print(feat[1])\n",
    "\n",
    "print(\"\\n label sample (shape={}):\".format(label[0].shape if hasattr(label[0], \"shape\") else ()))\n",
    "print(label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature shape dimensionality is `(18, 16)`, where `16` is the number of `input_features` and `18` is the possible `num_layers`. It follows this format:\n",
    "\n",
    "- `input_features = [\n",
    "    \"d_in1\": 0,\n",
    "    \"d_in2\": 1,\n",
    "    \"d_in3\": 2,\n",
    "    \"d_out1\": 3,\n",
    "    \"d_out2\": 4,\n",
    "    \"d_out3\": 5,\n",
    "    \"prec\": 6,\n",
    "    \"rf\": 7,\n",
    "    \"strategy\": 8,\n",
    "    \"layer_type\": 9,\n",
    "    \"activation_type\": 10,\n",
    "    \"filters\": 11,\n",
    "    \"kernel_size\": 12,\n",
    "    \"stride\": 13,\n",
    "    \"padding\": 14,\n",
    "    \"pooling\": 15]`\n",
    "\n",
    "This is the encoding:\n",
    "- `layer_type = [na = 0, dense=1, conv1d=2, conv2d=3, separableconv1d=4, separableconv2d=5, depthwiseconv1d=6, depthwiseconv2d=7, flatten=8, maxpooling=9, averagepooling=10]`\n",
    "- `activation_type = [na= 0, linear=1, relu=2, tanh=3, sigmoid=4, softmax=5]`\n",
    "- `padding = [na= 0, same=1, valid=2]`\n",
    "- Note: always doing pooling=2 when its a pooling layer (here the 2 is a literal representation)\n",
    "- can use zero-padding so if feature is undefined for a particular layer, just set it to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label shape dimensionality is `(5,)`, where `5` is the number of `output_features`. It follows this format:\n",
    "- `output_features = [\"WorstLatency_hls\", \"IntervalMax_hls\", \"FF_hls\", \"LUT_hls\", \"BRAM_18K_hls\", \"DSP_hls\"]`\n",
    "\n",
    "Meaning of each:\n",
    "- `WorstLatency_hls`: “cycles_max” in latency report\n",
    "- `IntervalMax_hls`: “interval_max” in latency_report (not yet)\n",
    "- `FF_hls`: “ff” in resource_report\n",
    "- `LUT_hls`: “lut” in resource_report\n",
    "- `BRAM_18K_hls`: “bram” in resource_report\n",
    "- `DSP_hls`: “dsp” in resource_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "Normalize the data:\n",
    "- take the log of things like the rf\n",
    "- Do simple transformations for features with very wide ranges (that are easy to undo)\n",
    "    - to get order 1 for all the input\n",
    "- Normalize everything, besides the encoded things\n",
    "    - embed activation type and layer type differently\n",
    "        - one-hot encoding\n",
    "        - its own embedding\n",
    "    - try as is for now though\n",
    "\n",
    "Normalize the outputs as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a dense model with 4 layers and the rest is \"-1\" padded:\n",
    "\n",
    "`feature sample (shape=(18, 16)):\n",
    "[[ 104.    0.    0.  128.    0.    0.   10. 2047.    1.    1.    0.    0.\n",
    "     0.    0.    0.    0.]\n",
    " [ 128.    0.    0.  128.    0.    0.   10.    1.    1.    0.    1.    0.\n",
    "     0.    0.    0.    0.]\n",
    " [ 128.    0.    0.  104.    0.    0.   10. 2047.    1.    1.    0.    0.\n",
    "     0.    0.    0.    0.]\n",
    " [ 104.    0.    0.  104.    0.    0.   10.    1.    1.    0.    1.    0.\n",
    "     0.    0.    0.    0.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]\n",
    " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.\n",
    "    -1.   -1.   -1.   -1.]]`\n",
    "\n",
    "#### And the model's corresponding resource utilization:\n",
    "` label sample (shape=(5,)):\n",
    "[ 3333. 19907. 26178.    58.    16.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "data_dir    = Path(\"../../../ddemler/dima_stuff/wa_remake/May_15_processed\")\n",
    "feat_np     = np.load(data_dir / \"combined_features.npy\")  # (N, 18, 16)\n",
    "label_np    = np.load(data_dir / \"combined_labels.npy\")    # (N,  5)\n",
    "\n",
    "pad_mask_np = np.all(feat_np == -1, axis=-1)               # (N,18)\n",
    "feat_np     = np.where(feat_np == -1, 0, feat_np)           # replace all -1 with 0\n",
    "\n",
    "# normalize \n",
    "skip_idxs    = {8, 9, 10, 15} # strategy, layer_type, activation_type, pooling\n",
    "norm_idxs    = [i for i in range(feat_np.shape[2]) if i not in skip_idxs]\n",
    "\n",
    "valid_rows = feat_np[~pad_mask_np].reshape(-1, feat_np.shape[2])  # (num_valid_rows, 16)\n",
    "\n",
    "means = valid_rows[:, norm_idxs].mean(axis=0)\n",
    "stds  = valid_rows[:, norm_idxs].std(axis=0)\n",
    "stds[stds < 1e-5] = 1.0\n",
    "\n",
    "# z-score norma\n",
    "feat_np[..., norm_idxs] = (\n",
    "    (feat_np[..., norm_idxs] - means) / stds\n",
    ")\n",
    "\n",
    "# convert to torch tensors\n",
    "feat     = torch.from_numpy(feat_np).float()                \n",
    "pad_mask = torch.from_numpy(pad_mask_np)                    \n",
    "label    = torch.from_numpy(label_np).float()               \n",
    "\n",
    "dataset    = TensorDataset(feat, pad_mask, label)\n",
    "N          = len(dataset)\n",
    "train_size = int(0.7 * N)\n",
    "generator  = torch.Generator().manual_seed(42)\n",
    "train_ds, test_ds = random_split(dataset, [train_size, N-train_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mfeat_path\u001b[49m\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please verify your data path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load and preprocess\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_path' is not defined"
     ]
    }
   ],
   "source": [
    "if not feat_path.exists():\n",
    "    print(f\"File not found: {feat_path}. Please verify your data path.\")\n",
    "else:\n",
    "    # Load and preprocess\n",
    "    feat_np = np.load(feat_path)                       # (N,18,16)\n",
    "    pad_mask_np = np.all(feat_np == -1, axis=-1)       # (N,18)\n",
    "    feat_np = np.where(feat_np == -1, 0, feat_np)\n",
    "\n",
    "    # Normalize all except categorical columns\n",
    "    skip_idxs = {8, 9, 10, 15}\n",
    "    norm_idxs = [i for i in range(feat_np.shape[2]) if i not in skip_idxs]\n",
    "\n",
    "    valid_rows = feat_np[~pad_mask_np].reshape(-1, feat_np.shape[2])\n",
    "    means = valid_rows[:, norm_idxs].mean(axis=0)\n",
    "    stds = valid_rows[:, norm_idxs].std(axis=0)\n",
    "    stds[stds < 1e-5] = 1.0\n",
    "    feat_np[..., norm_idxs] = (feat_np[..., norm_idxs] - means) / stds\n",
    "\n",
    "    # Column names\n",
    "    columns = [\n",
    "        \"d_in1\",\"d_in2\",\"d_in3\",\"d_out1\",\"d_out2\",\"d_out3\",\n",
    "        \"prec\",\"rf\",\"strategy\",\"layer_type\",\"activation_type\",\n",
    "        \"filters\",\"kernel_size\",\"stride\",\"padding\",\"pooling\"\n",
    "    ]\n",
    "\n",
    "    # Plot histograms for each normalized numeric feature\n",
    "    for idx in norm_idxs:\n",
    "        data = feat_np[~pad_mask_np, idx]\n",
    "        plt.figure()\n",
    "        plt.hist(data, bins=50)\n",
    "        plt.title(f\"Normalized '{columns[idx]}' distribution\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and define the architecture in `transformer.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rule4ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
